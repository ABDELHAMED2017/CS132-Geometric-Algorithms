{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix Factorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "internals": {},
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import laUtilities as ut\n",
    "import slideUtilities as sl\n",
    "import demoUtilities as dm\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "internals": {},
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
       "    display: None ! important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
    "    display: None ! important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "%Set up useful MathJax (Latex) macros.\n",
    "%See http://docs.mathjax.org/en/latest/tex.html#defining-tex-macros\n",
    "%These are for use in the slideshow\n",
    "$\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}$\n",
    "$\\newcommand{\\vx}{{\\mathbf x}}$\n",
    "$\\newcommand{\\hx}{\\hat{\\mathbf x}}$\n",
    "$\\newcommand{\\vbt}{{\\mathbf\\beta}}$\n",
    "$\\newcommand{\\vy}{{\\mathbf y}}$\n",
    "$\\newcommand{\\vz}{{\\mathbf z}}$\n",
    "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
    "$\\newcommand{\\vu}{{\\mathbf u}}$\n",
    "$\\newcommand{\\vv}{{\\mathbf v}}$\n",
    "$\\newcommand{\\vw}{{\\mathbf w}}$\n",
    "$\\newcommand{\\col}{{\\operatorname{Col}}}$\n",
    "$\\newcommand{\\nul}{{\\operatorname{Nul}}}$\n",
    "$\\newcommand{\\vb}{{\\mathbf b}}$\n",
    "$\\newcommand{\\va}{{\\mathbf a}}$\n",
    "$\\newcommand{\\ve}{{\\mathbf e}}$\n",
    "$\\newcommand{\\setb}{{\\mathcal{B}}}$\n",
    "$\\newcommand{\\rank}{{\\operatorname{rank}}}$\n",
    "$\\newcommand{\\vp}{{\\mathbf p}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}\n",
    "\\newcommand{\\vx}{{\\mathbf x}}\n",
    "\\newcommand{\\hx}{\\hat{\\mathbf x}}\n",
    "\\newcommand{\\vbt}{{\\mathbf\\beta}}\n",
    "\\newcommand{\\vy}{{\\mathbf y}}\n",
    "\\newcommand{\\vz}{{\\mathbf z}}\n",
    "\\newcommand{\\vb}{{\\mathbf b}}\n",
    "\\newcommand{\\vu}{{\\mathbf u}}\n",
    "\\newcommand{\\vv}{{\\mathbf v}}\n",
    "\\newcommand{\\vw}{{\\mathbf w}}\n",
    "\\newcommand{\\va}{{\\mathbf a}}\n",
    "\\newcommand{\\ve}{{\\mathbf e}}\n",
    "\\newcommand{\\vp}{{\\mathbf p}}\n",
    "\\newcommand{\\R}{{\\mathbb{R}}}\n",
    "\\newcommand{\\col}{{\\operatorname{Col}}}\n",
    "\\newcommand{\\nul}{{\\operatorname{Nul}}}\n",
    "\\newcommand{\\rank}{{\\operatorname{rank}}}\n",
    "\\newcommand{\\setb}{{\\mathcal{B}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Factorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A _factorization_ of a matrix $A$ is an equation that expresses $A$ as a product of two or more matrices.  \n",
    "\n",
    "$$A = BC.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_number": 4
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The essential difference with what we have done so far is that we have been given factors ($B$ and $C$) and computing $A$.  \n",
    "\n",
    "Today we will talk about situations where you are given $A$, and you want to find $B$ and $C$ -- that meet some conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 5
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a number of reasons one may want to factor a matrix.\n",
    "\n",
    "* Recasting $A$ into a form that makes computing with $A$ faster.\n",
    "* Recasting $A$ into a form that makes working with $A$ easier.\n",
    "* Recasting $A$ into a form that exposes important properties of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 6
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today we'll work with one particular factorization that addesses the first case.   Later one we'll study factorizations that address the other two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 7,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The factorization we will study is called the __LU Factorization.__  It is worth studying in its own right, and because it introduces the idea of factorizations, which we will study again later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 7,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Rationale for the LU Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 9
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say you want to solve a set of linear systems, all with the same coefficient matrix:\n",
    "\n",
    "$$A{\\bf x_1} = {\\bf b_1}$$\n",
    "$$A{\\bf x_2} = {\\bf b_2}$$\n",
    "$$\\dots$$\n",
    "$$A{\\bf x_p} = {\\bf b_p}$$\n",
    "\n",
    "There are $p$ equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 10
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naturally, you could solve these systems by first computing $A^{-1}$ and then computing:\n",
    "\n",
    "$${\\bf x_1} = A^{-1}{\\bf b_1}$$\n",
    "$${\\bf x_2} = A^{-1}{\\bf b_2}$$\n",
    "$$\\dots$$\n",
    "$${\\bf x_p} = A^{-1}{\\bf b_p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 11
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the computational cost of this?   \n",
    "\n",
    "Matrix Inversion requires approximately $2n^3$ flops (about three times as many flops as Gaussian Elimination.) \n",
    "\n",
    "This cost will dominate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 12
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, we could perform Gaussian Elimination on each of the systems.  \n",
    "\n",
    "This is probably worse, because then we have to perform $p \\cdot \\frac{2}{3}n^3$ flops.  \n",
    "\n",
    "Assuming $p > 3$, we are doing more work than if we invert $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 13
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we could solve all these systems while performing Gaussian Elimination only __once?__ \n",
    "\n",
    "That would be a win, as it would cut our running time by a factor of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 14
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The LU factorization allows us to do exactly this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 15,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we start to discuss the LU factorization, we need to introduce a powerful tool for performing factorizations, called _elementary matrices._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 15,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elementary Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 17
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall from the first and second lectures that the row reduction process consists of repeated applications of elementary row operations:\n",
    "\n",
    "* Exchange two rows\n",
    "* Multiply a row by a constant\n",
    "* Add a multiple of one row to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 18
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have much more theoretical machinery in our toolbox, we can make an important observation:\n",
    "\n",
    "__Every elementary row operation on $A$ can be performed by multiplying $A$ by a suitable matrix.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 19
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, an elementary row operation is a linear transformation!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 20
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Furthermore, the matrices that implement elementary row operations are particularly simple.  They are called __elementary matrices.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 21,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An elementary matrix is one that is obtained by __performing a single elementary row operation on the identity matrix.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 21,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example.__  Let\n",
    "\n",
    "$$E_1 = \\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\-4&0&1\\end{array}\\right],\\;\\; E_2 = \\left[\\begin{array}{rrr}0&1&0\\\\1&0&0\\\\0&0&1\\end{array}\\right],\\;\\; E_3 = \\left[\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\0&0&5\\end{array}\\right].$$\n",
    "\n",
    "Let's see what each matrix does to an arbitrary matrix $A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\g&h&i\\end{array}\\right].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 23
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ E_1A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\g-4a&h-4b&i-4c\\end{array}\\right]. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 24
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ E_2A = \\left[\\begin{array}{rrr}d&e&f\\\\a&b&c\\\\g&h&i\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 25
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ E_3A = \\left[\\begin{array}{rrr}a&b&c\\\\d&e&f\\\\5g&5h&5i\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 26,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly, left-multiplication by $E_1$ will add -4 times row 1 to row 3 (for any matrix $A$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 26,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Finding the Elementary Matrix.__\n",
    "\n",
    "In fact, you can look at this as follows.  \n",
    "\n",
    "Assume that some matrix $E$ exists that implements the operation \"add -4 times row 1 to row 3.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 28
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, for any matrix, $EI = E$ by the definition of $I$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 29
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But note that this equation also says:\n",
    "\n",
    "\"the matrix ($E$) that implements the operation 'add -4 times row 1 to row 3' is the one you get by performing this operation on $I$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 30
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus we have the following:\n",
    "\n",
    "__Fact.__ If an elementary row operation is performed on an $m\\times n$ matrix $A,$ the resulting matrix can be written as $EA$, where the $m\\times m$ matrix $E$ is created by performing the same row operation on $I_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 31,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One more thing: is an elementary matrix invertible?   \n",
    "\n",
    "Clearly, __yes:__ any row reduction operation can be reversed by another (related) row reduction operation.   \n",
    "\n",
    "So every row reduction is an invertible linear transformation -- so every elementary matrix is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 31,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question Time! Q12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 31,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The LU Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 34
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we will introduce the factorization \n",
    "\n",
    "$$ A = LU.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 35
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An LU factorization of $A$ constructs two matrices that have this structure:\n",
    "\n",
    "$$A = \\begin{array}{cc}\n",
    "\\left[\\begin{array}{cccc}1&0&0&0\\\\ *&1&0&0\\\\ *&*&1&0\\\\ *&*&*&1\\end{array}\\right]&\n",
    "\\left[\\begin{array}{ccccc}\\blacksquare&*&*&*&*\\\\0&\\blacksquare&*&*&*\\\\0&0&0&\\blacksquare&*\\\\0&0&0&0&0\\end{array}\\right]\\\\\n",
    "L&U\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Stars ($*$) denote arbitrary entries, and blocks ($\\blacksquare$) denote nonzero entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 36
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These two matrices each have a special structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 37
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First of all, $U$ is in row echelon form, and it has the same shape as $A$.   This is the \"upper\" matrix (hence its name $U$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 38
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second, $L$ is a lower triangular square matrix, and it has 1s on the diagonal.   This is called a _unit_ lower triangular matrix (hence its name $L$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 39
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The fact that $U$ is in row echelon form may suggest to you (correctly!) that we could get it from $A$ by a sequence of row operations.  \n",
    "\n",
    "For now, let us suppose that the row reductions that convert $A$ to $U$ only add a multiple of one row to another row __below__ it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 40
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, if you consider an elementary matrix that implements such a row reduction, you will see that it will have 1s on the diagonal, and an additional entry somewhere below the diagonal.  (Recall $E_1$ above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 41
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words, this sort of elementary matrix would actually be a unit triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 42
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if there is a sequence of row operations that convert $A$ to $U$, then there is a set of unit lower triangular elementary matrices $E_1, \\dots, E_p$ such that\n",
    "\n",
    "$$E_p\\cdots E_1A = U.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 43
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we know that all elementary matrices are invertible, and the product of invertible matrices is invertible, so:\n",
    "\n",
    "$$A = (E_p\\cdots E_1)^{-1} U = LU$$\n",
    "\n",
    "where\n",
    "\n",
    "$$L = (E_p\\cdots E_1)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 44
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's not hard to show that the product of unit lower triangular matrices is unit lower triangular.   It's also true that the inverse of a unit lower triangular matrix is unit lower triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 45
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can conclude that $L,$ as constructed from $(E_p\\cdots E_1)^{-1}$, is unit lower triangular.   Hence, we have defined the  LU decomposition based on Gaussian Elimination.\n",
    "\n",
    "We have rewritten Gaussian Elimination as:\n",
    "\n",
    "$$U = L^{-1}A$$\n",
    "\n",
    "and shown that the $L$ so defined is unit lower triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 46
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take stock of what this all means: the LU decomposition is a way of capturing the application of Gaussian Elimination to $A$.   It incorporates both the process of performing Gaussian Elimination, and the result:\n",
    "\n",
    "$L^{-1}$ captures the row reductions that transform $A$ to row echelon form.\n",
    "\n",
    "$U$ is the row echelon form of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 47,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note however that we have assumed that the Gaussian Elimination procedure only used certain row reductions -- in particular, no row interchanges.    (We'll deal with row interchanges later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 47,
     "slide_helper": "subslide_end",
     "slide_type": "subslide"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question Time! Q12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 47,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Computing $L^{-1}$.__\n",
    "\n",
    "Recall that the motivation for developing the LU decomposition is that it is more efficient than matrix inversion.   So we don't want to have to invert $L^{-1}$ in the standard way in order to find $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 50
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The book describes an algorithm that does this, and shows that one can invert $L^{-1}$ efficiently.  \n",
    "\n",
    "The basic idea is that $L^{-1}$ is a sequence of elementary row operations, and the inverse of $L^{-1}$ is the matrix $L$ that gives $L^{-1}L = I.$  \n",
    "\n",
    "So if we examine the sequence of elementary row operations, we can efficiently determine what $L^{-1}$ will give $I$ under those row operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 51
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This gives the following algorithm for LU factorization:\n",
    "    \n",
    "* Reduce $A$ to an echelon form $U$ by a sequence of row replacement operations, if possible.\n",
    "* Place entries in $L$ such that the _same sequence of row operations_ reduces $L$ to $I$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 52,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is the key observation: the step of reducing $A$ to echelon form is simply Gaussian Elimination.    If the second step can be done efficiently, then the whole LU factorization doesn't take any longer than Gaussian Elimination itself.  \n",
    "\n",
    "The fact is that constructing $L$ __can__ be done efficiently by a simple modification of Gaussian Elimination, and so LU decomposition takes time only $\\frac{2}{3}n^3.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 52,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the LU Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 52
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's return to the motivation for developing the LU factorization.\n",
    "\n",
    "We've seen that LU decomposition is essentially Gaussian Elimination, and as such, it doesn't take time much longer than Gaussian Elimination.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 55
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we want to show that, using the LU decomposition, that the system $A{\\bf x} = {\\bf b}$ can be solved for any ${\\bf b}$ in time that is proportional to $n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 56
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Informally, what we are going to do is to use $L$ to do a special, very efficient version of the forward step of Gaussian Elimination, and then use $U$ in the usual way to do backsubstitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 57
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can write these two steps concisely as follows:\n",
    "\n",
    "When $A = LU$, the equation $A{\\bf x} = {\\bf b}$ can be written as $L(U{\\bf x}) = {\\bf b}.$  \n",
    "\n",
    "Let's take this apart, and write ${\\bf y}$ for $U{\\bf x}$.  Then we can find ${\\bf x}$ by solving the pair of equations:\n",
    "\n",
    "$$L{\\bf y} = {\\bf b},$$\n",
    "$$U{\\bf x} = {\\bf y}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 58
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea is that we first solve $L{\\bf y} = {\\bf b}$ for ${\\bf y},$ then solve $U{\\bf x} = {\\bf y}$ for ${\\bf x}.$\n",
    "\n",
    "In a sense, this corresponds to first performing the forward step of Gaussian Elimination (but in a specially streamlined, efficient way) and then performing the backwards (backsubstitution) step in the usual (efficient) fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 59
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-926dcbf00f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# image credit: Lay, 4th edition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhide_code_in_slideshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/Lay-fig-2-5-2.jpeg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sl' is not defined"
     ]
    }
   ],
   "source": [
    "# image credit: Lay, 4th edition\n",
    "sl.hide_code_in_slideshow()\n",
    "display(Image(\"images/Lay-fig-2-5-2.jpeg\", width=550))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 60,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The key observation: __each equation is fast to solve__ because $L$ and $U$ are each triangular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 60,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example.__\n",
    "\n",
    "Given the following LU decomposition of $A$:\n",
    "\n",
    "$$A = \\left[\\begin{array}{rrrr}3&-7&-2&2\\\\-3&5&1&0\\\\6&-4&0&-5\\\\-9&5&-5&12\\end{array}\\right] = \n",
    "\\left[\\begin{array}{rrrr}1&0&0&0\\\\-1&1&0&0\\\\2&-5&1&0\\\\-3&8&3&1\\end{array}\\right]\n",
    "\\left[\\begin{array}{rrrr}3&-7&-2&2\\\\0&-2&-1&2\\\\0&0&-1&1\\\\0&0&0&-1\\end{array}\\right] = LU$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 62
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use this LU factorization of $A$ to solve $A{\\bf x} = {\\bf b}$, where ${\\bf b} = \\left[\\begin{array}{r}-9\\\\5\\\\7\\\\11\\end{array}\\right].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 63
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Solution.__  To solve $L{\\bf y} = {\\bf b},$ note that the arithmetic takes place only in the augmented column (column 5).   The zeros below each pivot in $L$ are created automatically by the choice of row operations.\n",
    "\n",
    "$$[L\\;\\; {\\bf b}] = \\left[\\begin{array}{rrrrr}1&0&0&0&-9\\\\-1&1&0&0&5\\\\2&-5&1&0&7\\\\-3&8&3&1&11\\end{array}\\right] \\sim\n",
    "\\left[\\begin{array}{rrrrr}1&0&0&0&-9\\\\0&1&0&0&-4\\\\0&0&1&0&5\\\\0&0&0&1&1\\end{array}\\right] = [I\\;{\\bf y}].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 64,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, for $U{\\bf x} = {\\bf y}$ (the \"backward\" phase) the row reduction is again streamlined:\n",
    "\n",
    "$$[U\\;\\;{\\bf y}] =  \\left[\\begin{array}{rrrrr}3&-7&-2&2&-9\\\\0&-2&-1&2&-4\\\\0&0&-1&1&5\\\\0&0&0&-1&1\\end{array}\\right] \\sim\n",
    "\\left[\\begin{array}{rrrrr}1&0&0&0&3\\\\0&1&0&0&4\\\\0&0&1&0&-6\\\\0&0&0&1&-1\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 64,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Analysis.__\n",
    "\n",
    "Both the forward and backward phases of solving a system as $L(U{\\bf x}) = {\\bf y}$ require approximately $2n^2$ flops. \n",
    "\n",
    "Therefore the time consuming step is actually doing the factorization, which as we've seen is essentially Gaussian Elimination, and therefore requires $\\frac{2}{3}n^3$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 66,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hence we have found that by using the LU decomposition, one can solve a series of systems all involving the same $A$ in $\\frac{2}{3}n^3$ flops, \n",
    "\n",
    "while doing Gaussian Elimination would require $\\frac{2}{3}n^3$ flops _for each system_, \n",
    "\n",
    "and using the matrix inverse would require $2n^3$ flops to invert the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 66,
     "slide_type": "subslide"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pivoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 68
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Up until now we have assumed that the Gaussian Elimination we used to define $U$ only involves adding a multiple of a row to a row below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 69
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, in real situations, we sometime need to exchange two rows.\n",
    "\n",
    "One reason is, as we know, that if the current row has a zero in the pivot position, we need to exchange it with a row that does not have a zero in the pivot position.\n",
    "\n",
    "But there is another, more subtle reason having to do with numerical accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 70
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We make the following observation: in general, we would like to avoid __dividing by a small number.__\n",
    "\n",
    "Here is why.   Consider the problem of computing $a/b$ where $a$ and $b$ are scalars.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 71
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say there is some small error in the value of $a$, call it $\\epsilon.$   \n",
    "\n",
    "That means that what we are really computing is $(a+\\epsilon)/b = a/b + \\epsilon/b$. \n",
    "\n",
    "Note that $a/b$ is the correct value, but that what we compute is off by $\\epsilon/b$.  Now, if $b$ is a very small number, then the error in the result ($\\epsilon/b$) will be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 72
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hence we would like to avoid dividing by small numbers whenever possible.  \n",
    "\n",
    "Now: note that in performing Gaussian Elimination, we divide each row by the value of its pivot.\n",
    "\n",
    "What this suggests is that we would like to avoid having small pivots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 73
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a simple way to address this.   In processing any particular row, we can avoid having a small pivot by interchanging the current row with one of those below it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 74
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We would like to exchange the current row with the row that __has the largest absolute value of its pivot.__  This algorithmic technique is called \"partial pivoting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 75
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, a row interchange is an elementary row operation, and can be implemented by an elementary matrix.  This elementary matrix is the identity with its corresponding rows interchanged.   \n",
    "\n",
    "An elementary matrix that exchanges rows is called a _permuation_ matrix.  The product of permutation matrices is a permutation matrix.  \n",
    "\n",
    "Hence, the net result of all the partial pivoting done during Gaussian Elimination can be expressed in a single permutation matrix $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 76
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that the final factorization of $A$ is:\n",
    "\n",
    "$$A = P L U.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "internals": {
     "frag_helper": "fragment_end",
     "frag_number": 77,
     "slide_helper": "subslide_end"
    },
    "slide_helper": "slide_end",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can read this equation two ways:\n",
    "\n",
    "1. $A$ is the product of a unit lower triangular matrix $L$ and an echelon form matrix $U$, and the rows of their product have been reordered according to the permulation $P$.  This is $A = P(LU).$\n",
    "2. $A$ is the product of a _permuted_ lower triangular matrix $PL$ and an echelon form matrix $U$.  This is $A = (PL)U.$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
